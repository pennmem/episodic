{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2d18bf7-3074-4bed-aca4-d9076a7dc581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import sys\n",
    "import os.path as op\n",
    "from time import time\n",
    "from glob import glob\n",
    "import warnings\n",
    "from importlib import reload\n",
    "from cluster_helper.cluster import cluster_view\n",
    "\n",
    "# Personal\n",
    "sys.path.append('/home1/dscho/code/projects')\n",
    "from brandon import code_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377a8bc-5dd2-4697-9401-6960ae279d6a",
   "metadata": {},
   "source": [
    "### We will parallelize the function _save_some_file()_ in _/home1/dscho/code/projects/code_examples_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a6ef9-92d4-4839-a604-6ec098b0051a",
   "metadata": {},
   "source": [
    "---\n",
    "First let's take a look at the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e2734c5-69fd-4b49-9dd8-f3c8ca3268e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_examples.save_some_file??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b3f578-9092-4067-8da3-394f7c0f7c7c",
   "metadata": {},
   "source": [
    "---\n",
    "Here's what it looks like to run this function in serial (i.e. one function call at a time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84e31d37-6c2f-4339-89a4-a95ddf383f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'daniel'\n",
    "\n",
    "output_file = code_examples.save_some_file(name)\n",
    "\n",
    "print('Saved {}'.format(output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "286544e9-c22b-4d26-9825-08aa45429d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa85157-aa7f-4503-a856-e62d10092bab",
   "metadata": {},
   "source": [
    "---\n",
    "Okay, now time to parallelize.\n",
    "\n",
    "There are two steps to this process:\n",
    "1. First, we write a simple \"calling function,\" that takes a single argument as input, \\\n",
    "   and calls the function that we want to parallelize with a specific set of input values.\n",
    "   - Although cluster_helper requires you to pass only a single argument as input,\\ \n",
    "     this object can be anything...including e.g. a list or dictionary that contains \\\n",
    "     more than one piece of info. But basically, you want to pass whichever parameters \\\n",
    "     are going to vary from one instance of the parallelized function to another. \\\n",
    "     (E.g. a subject ID, EEG file path, wavelet frequency, etc.)\n",
    "   - When this function is called during parallelization, it cannot access \\\n",
    "     any modules that have been imported within the Jupyter notebook. So you must \\\n",
    "     reimport any module-dependent functions at the top of the calling function. \\\n",
    "     Forgetting about this rule is a common mistake behind many parallelization fails :_)\n",
    "2. Next, we initialize the parallelization process using some boilerplate code \\\n",
    "   that calls the cluster_helper function *cluster_view()*, and gives it (1) the calling \\\n",
    "   function that we just wrote and (2) a list of things that we want to pass to it as input.\n",
    "   - E.g., if each run of the calling function takes a single subject ID as input, \\\n",
    "     then you would pass a list of subject IDs that you want to run the function on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4793aafa-ea6f-44bf-a16f-c180ddb400cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the calling function\n",
    "def save_some_file_parallel(name):\n",
    "    \"\"\"I live to call some_file_name()\"\"\"\n",
    "    import sys\n",
    "    sys.path.append('/home1/dscho/code/projects')\n",
    "    from brandon import code_examples\n",
    "    \n",
    "    output_file = code_examples.save_some_file(name)\n",
    "    \n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7cd79601-ad57-4214-b6cb-ee3b3c4ff031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of names to process.\n",
    "input_items = ['brandon', 'joey', 'john sakon']\n",
    "\n",
    "print('{} names to process'.format(len(input_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d17f828c-0757-4e2b-85f5-34c3928c283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize your function\n",
    "calling_function = save_some_file_parallel # the name of your calling function\n",
    "n_jobs = len(input_items) # the number of jobs we are going to parallelize, in total\n",
    "max_jobs = 100 # this is maximum number of jobs that we will allow to run at once. \n",
    "               # check qstat -g c before choosing, but generally don't go over 100,\n",
    "               # and use <100 if there are fewer than 250 cores available.\n",
    "               # if max_jobs < n_jobs, then some jobs will go in the queue and run\n",
    "               # automatically as soon as other jobs finish running.\n",
    "cores_per_job = 1 # this is how many cores to use per job. pretty much should always be 1\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "print('Running code for {} operations.\\n'.format(n_jobs))\n",
    "start_time = time()\n",
    "try:\n",
    "    with cluster_view(scheduler=\"sge\", queue=\"RAM.q\", num_jobs=min((n_jobs, max_jobs)), cores_per_job=cores_per_job) as view:\n",
    "        _ = view.map(calling_function, input_items)\n",
    "except OSError:    \n",
    "    print('Done in {:.1f}s'.format(time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103ae86-b2bf-46c6-8628-6d53db2890b6",
   "metadata": {},
   "source": [
    "---\n",
    "Let's see if our code worked..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4281f6f7-892c-403b-ac13-a0ddaef1a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = glob(op.join(op.dirname(output_file), 'message_for_*'))\n",
    "\n",
    "print('Found {} output files'.format(len(output_files)),\n",
    "      '---------------------', sep='\\n', end='\\n'*2)\n",
    "    \n",
    "for fpath in output_files:\n",
    "    print(op.basename(fpath))\n",
    "    with open(fpath, 'r') as file:\n",
    "        print(file.read(), end='\\n'*2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memlab",
   "language": "python",
   "name": "memlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
